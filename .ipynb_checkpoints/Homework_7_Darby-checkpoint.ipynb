{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 for Niko Darby "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raschka performs topic modeling on the IMDb dataset in cells 45-50 of ch08.ipynb. In the related section of the text, he discusses three important hyperparameters. Investigate the sensitivity to those hyperpameters in Colab. Demonstrate and describe your results in a single notebook with sufficient detail that they can be reproduced. Include in that notebook a paragraph summarizing your major findings.\n",
    "\n",
    " - You're welcome to work in teams.\n",
    "\n",
    "    - If you work in teams and decide to make a single submission, then identify all team members in the notebook and have each team member submit a link to that notebook.\n",
    "\n",
    "## Be prepared to present your notebook and discuss your results during class.\n",
    "\n",
    "### <b> Source: </b> https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "### <b>Disclaimer:</b> Do know that the code below is not created by Niko Darby. The code below is created by Raschka and Niko Darby is simply manipulating then explaining the code for a homework assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data from Raschka: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1      OK... so... I really like Kris Kristofferson a...          0\n",
       "2      ***SPOILER*** Do not read this, if you think a...          0\n",
       "3      hi for all the people who have seen this wonde...          1\n",
       "4      I recently bought the DVD, forgetting just how...          0\n",
       "...                                                  ...        ...\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\admin\\Desktop\\movie_data.csv', encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact copy of Raschka's Notebook for reference:\n",
    "#### (Source code referenced in each cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
      "\n",
      "Horror movie #3:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify and inspect the hyperparameters of <i>max_features</i>, <i> max_df</i>, and <i> n_components </i>: \n",
    "#### (Code cells from Raschka will be recapitulated below and the hyper parameters will then be edited. Afterwards, a short summary of each difference will be noted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simply altering <i> max_features </i> this round: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=100)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "comedy family minutes fun short\n",
      "Topic 2:\n",
      "book special effects read budget\n",
      "Topic 3:\n",
      "music original version american play\n",
      "Topic 4:\n",
      "horror death script fan budget\n",
      "Topic 5:\n",
      "woman different beautiful performance played\n",
      "Topic 6:\n",
      "ending star wasn tv kind\n",
      "Topic 7:\n",
      "believe dvd watched let ll\n",
      "Topic 8:\n",
      "action worst nice dead looks\n",
      "Topic 9:\n",
      "guy black men john girl\n",
      "Topic 10:\n",
      "series war tv shows role\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "A Movie about a bunch of some kind of filmmakers, who want to make a documentary on a new kind of surfing in shark-infested waters. As an absolute fan of movies including some kind of vicious animals or monsters, I thought this might be my kind of movie... it wasn't!!! This should be more of a guide ...\n",
      "\n",
      "Horror movie #2:\n",
      "It's unfortunate that you can't go any lower than one star. Prior to watching The Wicker Man, I had considered Aliens 3 to be the only movie that would actually merit negative stars. In all fairness, The Wicker Man doesn't detract from the enjoyment of an earlier film, but the fact remains that my c ...\n",
      "\n",
      "Horror movie #3:\n",
      "`Rock star' is not on its way to any `stairway to heaven' category as one of the best rock films of all time, but it does make you `jump' from time to time because of its high-level energy. The film's theme is on a die-hard rock group fanatic who actually becomes the lead singer of his favorite band ...\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Altering the <i> max_features </i> : \n",
    "\n",
    "#### Upon altering the <i> max_features </i> of the CountVectorizer() function from SciKitLearn, the amount of dimensions/features were altered as expected. Also, the output is quite different from the initial output witnessed from Raschka.  This is due to the <i> max_features </i> being reduced. \n",
    "\n",
    "#### In reference to the first print-out, upon the features being reduced the amount of data that the algorithm can learn from is minimized, thus leaving a different set of options (or topics) to return. Furthermore, transitioning to the second print-out that focuses on horror movies:  with less features arises less accuracy. Initially, the data contained 5000 features in Raschka's example and the classification of horror films was immensely accurate. In this alternative version, there are only 100 features and the accuracy of the classification has been decreased tremendously.\n",
    "\n",
    "#### For instance, upon looking at \"Horror Movie #3\" , it discusses a \"Rock Star\" instead of a scenario related to horror. Additionally, \"Horror Movie #1\" entails a documentary about dangerous surfing - yet, this is nowhere near associated with the common definition of a horror film.  In essence, by reducing the number of <i> max_features </i> in the CountVectorizer() function, one will ultimately reduce the learning accuracy of their model and thus receive inaccurate genre classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simply altering <i> max_df </i> this round: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i> max_df </i> of 90% : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.9,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "film br films war american\n",
      "Topic 2:\n",
      "film movie bad like just\n",
      "Topic 3:\n",
      "br film man life story\n",
      "Topic 4:\n",
      "movie people like just life\n",
      "Topic 5:\n",
      "br movie story film book\n",
      "Topic 6:\n",
      "br like just series good\n",
      "Topic 7:\n",
      "br role cast best john\n",
      "Topic 8:\n",
      "horror movie film good great\n",
      "Topic 9:\n",
      "movie br just good like\n",
      "Topic 10:\n",
      "film great movie story love\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "The actresses bra in a changing room--well I guess they are preparing young children for changing room time? (Boys you must close your eyes at that scene A humongous bra (34C which definitely neither of the actresses size) dangling and supposedly talking--oh don't worry if your son takes your bra th ...\n",
      "\n",
      "Horror movie #2:\n",
      "** WARNING - CONTAINS SPOILERS! **<br /><br />First of all, I would like to say that I really liked this game. I got it for Christmas after two months of dropping hints to my parents. I am glad that I did that.<br /><br />First off, I would like to say that the single player was very good. The first ...\n",
      "\n",
      "Horror movie #3:\n",
      "Like many here I grew up with Scooby-Doo. Unlike many here who did, I love this show! I think that it has been very well done and thought through. Everything about it marks it as a spin-off which isn't meant to be taken seriously. The formula is simple - it is a parody of other cartoons with a singl ...\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <i> max_df </i> of 50%: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.5,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "series tv time original episode\n",
      "Topic 2:\n",
      "great music best love like\n",
      "Topic 3:\n",
      "man woman like wife just\n",
      "Topic 4:\n",
      "characters story character films plot\n",
      "Topic 5:\n",
      "like just good really think\n",
      "Topic 6:\n",
      "bad just like don really\n",
      "Topic 7:\n",
      "good story character cast director\n",
      "Topic 8:\n",
      "action good man game best\n",
      "Topic 9:\n",
      "horror good like films effects\n",
      "Topic 10:\n",
      "life people story love world\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "I saw this movie once, and I thought it was OK. Then my friends at work said \"Watch it again, it's better\". So I did. And to my surprise, it was WORSE on the second time! There's a word limit, so I'm going to get the ball rolling here.<br /><br />-The bombing scenes were all so stupid. Why on earth  ...\n",
      "\n",
      "Horror movie #2:\n",
      "That's what me and my friends kept asking each other throughout this entire flick. We couldn't believe how stupid it was! I think somebody shot this on their camcorder at home and snuck it into the movie store and put it on the shelf as a joke to see if anybody would ever pick it up. Well, I guess t ...\n",
      "\n",
      "Horror movie #3:\n",
      "OK, I admit I watched this movie on Mystery Science Theater 3000 (which I am a huge fan of), but I am not one of those people who automatically gives an MST3K movie a 1/10 rating. Although I hate many of the movies they play, and some are among of the worst movies I've ever seen, I have actually bee ...\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Altering the <i> max_df </i> : \n",
    "\n",
    "#### According to the documentation, the <i> max_df </i> is responsible for eradicating any terms that appear more than the percentage provided to the hyperparameter (source 1). For instance, if the word \"the\" appears more than 90% of the time, it will be diminished from the dataset. \n",
    "\n",
    "#### Resultantly,  as the percentage of the hyperparameter gets closer to 100%, the data becomes more inaccurate.  Examine the <i> max_df </i> of 90% posted above. In this sector, the topics are immensely inaccurate and has random words.\n",
    "\n",
    "#### In addition, the short reviews of the horror movie genre are also inaccurate. This is due to particular words being diminished from the dataset. Consequently, this inhibits the algorithm from having the ability to truly learn from the data and master the categorical classifications. Once the words are dropped as a result of appearing 90% of the time, this causes the algorithm to lose knowledge. As more evidence, look at what happens above when the <i> max_df </i> is reduced to 50% - the topics become a bit more accurate although the horror movies do not.  Yet, when the <i> max_df </i> is reduced to 10%, the topics and horror movie reviews are very accurate and concise. \n",
    "\n",
    "#### Overall, with a lower diminishment of frequent terms arises a higher classification accuracy. Thus, it is essential to keep the <i> max_df </i> at a lower percentage unless there is a scenario where it is crucial to increase the percentage. \n",
    "\n",
    "## Source: \n",
    "### Source 1: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simply altering <i> n_components </i> this round: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=4,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "horror worst minutes guy script\n",
      "Topic 2:\n",
      "family woman girl father mother\n",
      "Topic 3:\n",
      "role performance john comedy played\n",
      "Topic 4:\n",
      "war book series action music\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "This review is long overdue, since I consider A Tale of Two Sisters to be the single greatest film ever made. I'll put this gem up against any movie in terms of screenplay, cinematography, acting, post-production, editing, directing, or any other aspect of film-making. It's practically perfect in al ...\n",
      "\n",
      "Horror movie #2:\n",
      "Inarguably one of the most interesting filmmakers of the last 50 years, Werner Herzog has been pushing the boundaries of cinema perhaps more so than any other commercial filmmaker. I've been acquainted with Herzog for a few decades now and I've never not been impressed by both the man and his work.  ...\n",
      "\n",
      "Horror movie #3:\n",
      "\"Nobi\" or \"Fires On the Plain\" is a film that is so excellent on so many levels, that not enough good things can be said about it. My only regret is that I was not able to see this 1959 film sooner.<br /><br />Being something of a film purist, I tend to look at films for their artistic merits based  ...\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb\n",
    "\n",
    "horror = X_topics[:, 3].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Altering the <i> n_components </i> : \n",
    "\n",
    "#### The <i> n_components </i> hyperparameter of the Latent Dirichlet Allocation function simply determines the number of topics to return (Source 2). For example, as performed above, if one were to alter the <i> n_components </i> to the number four (4), then this would cause only 4 topics to be returned. Altering the number of topics does have an effect on the horror movie review data. The indexing for the horror movies decreased and the accuracy of the horror movie reviews decreased as well. Thus, upon reducing the number of topics, this in turn reduced the accuracy of the algorithm in the scenario of the movie genre reviews. \n",
    "\n",
    "### Source: \n",
    "### Source 2: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
